# ARP Base Platform

**ARP Base Platform**은 Meta Quest 3 환경에서 현실 공간과 가상 객체를 정합하여 상호작용할 수 있도록 설계된 **Mixed Reality 애플리케이션**입니다.  
사용자는 현실의 스크린 위치를 기준으로 가상의 Plane(스크린)을 조정하고, 로컬 PC와의 네트워크 통신을 통해 실제 버튼 입력 및 3D 오브젝트 출현 등의 동작을 체험할 수 있습니다.

---

## 🏁 실행 개요

### 1️⃣ 공간 스캔
- 앱 실행 전, **Meta Quest 3의 공간 스캔 기능**을 통해 주변 환경을 인식해야 합니다.
- 스캔이 완료되면 앱 실행 시 주변 공간이 **blue effect mesh** 로 시각화됩니다.

### 2️⃣ Plane(가상 스크린) 조작
- Plane은 **현실 스크린의 프록시** 역할을 합니다.
- **오른쪽 컨트롤러 조이스틱**으로 크기를 조정할 수 있습니다.
- **A 버튼**을 눌러 Plane을 실제 공간에 **앵커(anchor)** 합니다.
- 현실 스크린과 Plane이 일치하도록 정렬해야 합니다.
- 정렬 후에는 **효과 메쉬가 자동으로 꺼집니다.**

### 3️⃣ 설정 패널
- 왼손바닥을 바라보면 **Setting 버튼**이 표시됩니다.  
- 버튼을 누르면 **설정 패널**이 사용자 앞에 표시됩니다.
- 설정 패널에서는 다음 항목을 조정할 수 있습니다:
  - IP 주소 입력  
  - 가상 스크린 표시 여부  
  - 네트워킹 기능 사용 여부 (로컬 PC 연결 제어)

---

## 💻 로컬 서버 연동

### ▶ `app.py` 실행 (웹 UI 버전)
- Python 서버를 실행하면 웹 브라우저에 **3개의 버튼이 있는 페이지**가 열립니다.
- 사용자가 HMD 상의 가상 스크린을 통해 버튼을 터치하면,  
  그 입력이 **로컬 PC로 전송되어 실제 버튼이 클릭**됩니다.
- 어떤 버튼이 눌렸는지 정보가 **PC → HMD로 전송**되어,  
  앱에서 **지정된 3D 오브젝트가 등장**합니다.

> 즉, `app.py`는 **가상 스크린을 통한 상호작용 예제**입니다.

### ▶ `app_noweb.py` 실행 (터미널 버전)
- 웹 인터페이스 없이 터미널에서 **2D 터치 좌표 데이터**를 출력합니다.
- 사용자가 가상 스크린을 터치한 위치를 **픽셀 단위로 실시간 확인**할 수 있습니다.

> `app_noweb.py`는 **터치 포인트 데이터를 수집하거나 디버깅용으로 활용**할 수 있습니다.

---

## ⚙️ 실행 방법

### 1️⃣ HMD 설정
1. Meta Quest 3에서 공간 스캔 완료  
2. 앱 실행 후 Plane을 조정하고 A키로 앵커 고정  
3. 왼손바닥의 Setting 버튼으로 네트워크 및 스크린 표시 설정

### 2️⃣ 로컬 PC 설정
```bash
# Flask 설치 (없다면)
pip install Flask #conda/pip 사용자
# 또는
uv add flask #uv 사용자
# Python 서버 실행
python app.py
# 또는
python app_noweb.py
